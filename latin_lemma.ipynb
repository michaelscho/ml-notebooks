{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0f7c2c3b7a7b4a9b",
      "metadata": {},
      "source": [
        "# Lemmatizing Latin with a Seq2Seq Transformer (`mschonhardt/latin-lemmatizer`)\n",
        "\n",
        "This notebook shows how to use the Hugging Face Transformers pipeline with the lemmatization model:\n",
        "\n",
        "- Lemmatizer: `mschonhardt/latin-lemmatizer`\n",
        "\n",
        "The model is a `text2text-generation` (Seq2Seq) model that takes Latin text as input and returns a lemmatized version as generated text.\n",
        "Model can be found on [Hugging Face](https://huggingface.co/mschonhardt/latin-lemmatizer) and [Zenodo](https://doi.org/10.5281/zenodo.18632650).\n",
        "\n",
        "\n",
        "![](https://zenodo.org/badge/DOI/10.5281/zenodo.18632650.svg)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2a3f2b2e8e74a2a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# If needed, install dependencies\n",
        "# !pip install -U transformers torch sentencepiece accelerate pandas tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3df51a1a4c684e5e",
      "metadata": {},
      "source": [
        "## 1) Setup\n",
        "Select device, import libraries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8f0b3b3fb9a4aa5",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import pipeline\n",
        "\n",
        "# Device selection for transformers pipeline:\n",
        "# device=0 uses first CUDA GPU; device=-1 uses CPU.\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "device_name = torch.cuda.get_device_name(0) if device == 0 else \"CPU\"\n",
        "print(f\"Using device: {device_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d2bd082b8b34b38",
      "metadata": {},
      "source": [
        "## 2) Load the lemmatizer pipeline\n",
        "We use `text2text-generation` because the model generates the lemmatized text as output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1d223a8dc0b4c16",
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_LEMMATIZER = \"mschonhardt/latin-lemmatizer\"\n",
        "\n",
        "print(\"Loading lemmatizer pipeline...\")\n",
        "lemmatizer_pipe = pipeline(\n",
        "    task=\"text2text-generation\",\n",
        "    model=MODEL_LEMMATIZER,\n",
        "    device=device,\n",
        ")\n",
        "print(\"Loaded:\", MODEL_LEMMATIZER)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac38c293a7b3487e",
      "metadata": {},
      "source": [
        "## 3) Lemmatize a single text\n",
        "The pipeline returns a list of dicts; the lemmatized string is in `generated_text`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f6fd07d3f1a43c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "example_text = \"In nomine sanctae et individuae trinitatis .\"\n",
        "\n",
        "result = lemmatizer_pipe(\n",
        "    example_text,\n",
        "    max_length=512,\n",
        "    truncation=True,\n",
        ")\n",
        "\n",
        "lemmatized = result[0][\"generated_text\"]\n",
        "print(\"INPUT :\", example_text)\n",
        "print(\"OUTPUT:\", lemmatized)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f1b8dbb5c0b4d8a",
      "metadata": {},
      "source": [
        "## 4) Lemmatize multiple texts efficiently (batching)\n",
        "For larger corpora you should batch inputs. Adjust `batch_size` based on your VRAM.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b53fb6b9b6f42b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "def lemmatize_texts(texts, batch_size=32, max_length=512):\n",
        "    out = []\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Lemmatizing\"):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        results = lemmatizer_pipe(\n",
        "            batch,\n",
        "            max_length=max_length,\n",
        "            truncation=True,\n",
        "        )\n",
        "        out.extend([r[\"generated_text\"] for r in results])\n",
        "    return out\n",
        "\n",
        "texts = [\n",
        "    \"In nomine sanctae et individuae trinitatis .\",\n",
        "    \"Quod infames uocentur qui ex consanguineis nascuntur .\",\n",
        "    \"Si quis clericus furtum fecerit , deponatur .\",\n",
        "    \"Arma virumque cano , Troiae qui primus ab oris .\",\n",
        "]\n",
        "\n",
        "lemmatized_texts = lemmatize_texts(texts, batch_size=16, max_length=512)\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"text\": texts,\n",
        "    \"lemmatized_text\": lemmatized_texts,\n",
        "})\n",
        "df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5aa9d507c5f84f35",
      "metadata": {},
      "source": [
        "## 5) (Optional) Export results\n",
        "Example: save as CSV for later reuse.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1bf37d40f0644aa",
      "metadata": {},
      "outputs": [],
      "source": [
        "# df.to_csv(\"latin_lemmatization_demo.csv\", index=False)\n",
        "# print(\"Saved latin_lemmatization_demo.csv\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv-jupyter",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

